{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from PIL import Image\n",
    "\n",
    "# zeige keine Warnungen an\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.ocr_pipeline import OCRPreprocessor, OCRPostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz initialisieren\n",
    "dataset = DatasetDict.load_from_disk(\"../data/interim_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "class OCRPipeline:\n",
    "    def __init__(self, image: Union[np.ndarray, Image.Image]):\n",
    "        \"\"\"OCR-Pipeline zu Vorbereitung des Dokumentes, \n",
    "        Extraktion des Textes und Aufbereitung des extrahierten Textes.\n",
    "\n",
    "        Args:\n",
    "            Args:\n",
    "            image (Union[np.ndarray, Image.Image]): Das Eingangsbild als NumPy-Array oder PIL.Image.Image.\n",
    "        \"\"\"\n",
    "        self.raw_image = image\n",
    "        self.preprocessed_image = None\n",
    "        self.ocr_output = \"\"\n",
    "\n",
    "    def preprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPreprocessor an, speichert das verarbeitete Bild.\"\"\"\n",
    "        preprocessor = OCRPreprocessor(self.raw_image)\n",
    "        preprocessor.cropping(buffer_size=10)\n",
    "        preprocessor.to_gray()\n",
    "        preprocessor.correct_skew()\n",
    "        preprocessor.sharpen(kernel_type=\"laplace_standard\")\n",
    "        preprocessor.opening(kernel=(1,1), iterations=2)\n",
    "        preprocessor.power_law_transform(gamma=2)\n",
    "        self.preprocessed_image = preprocessor.get_image()\n",
    "\n",
    "    def extract_text(self) -> None:\n",
    "        \"\"\"Wendet PyTesseract auf das vorverarbeitete Bild an und speichert den Text.\"\"\"\n",
    "        self.ocr_output = pytesseract.image_to_string(self.preprocessed_image)\n",
    "\n",
    "    def postprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPostProcessor auf den extrahierten Text an.\"\"\"\n",
    "        if self.ocr_output.strip():  # Prüft, ob `ocr_output` nicht leer ist\n",
    "            postprocessor = OCRPostProcessor(self.ocr_output)\n",
    "            # Anwenden verschiedener Methoden\n",
    "            postprocessor.identify_language()\n",
    "            postprocessor.remove_special_characters()\n",
    "            postprocessor.lowercase()\n",
    "            postprocessor.remove_stopwords()\n",
    "            postprocessor.remove_extra_spaces()\n",
    "            \n",
    "            # Aufbereiteten OCR-Output extrahieren\n",
    "            self.ocr_output = postprocessor.get_text()\n",
    "        else:\n",
    "            self.ocr_output = \"no text found in document image with ocr!\"\n",
    "\n",
    "    def get_output(self):\n",
    "        \"\"\"Gibt den aufbereiteten OCR-Output zurück.\"\"\"\n",
    "        return self.ocr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for split in dataset.keys():\n",
    "    #dataset[split] = dataset[split].add_column(\"text\", [\"\"]*len(dataset[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "## Erstellen eines Subsets mit 50 Beispielen aus einem Split\n",
    "subset = dataset[\"train\"].select(range(50))\n",
    "\n",
    "#subset = subset.remove_columns(['text'])\n",
    "\n",
    "def apply_ocr(batch):\n",
    "    texts = []\n",
    "    for image in batch[\"image\"]:\n",
    "        ocr_pipeline = OCRPipeline(image)\n",
    "        ocr_pipeline.preprocess()\n",
    "        ocr_pipeline.extract_text()\n",
    "        ocr_pipeline.postprocess()\n",
    "        texts.append(ocr_pipeline.get_output())\n",
    "    batch[\"text\"] = texts\n",
    "    \n",
    "    del texts, ocr_pipeline\n",
    "    gc.collect()\n",
    "    return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ocr_to_dataset(dataset: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Diese Methode wendet die OCR (Optical Character Recognition) auf alle Bilder in jedem Split (train, validation, test) eines Huggingface-Datensatzes an und fügt ein neues Feature hinzu, das den erkannten Text enthält.\n",
    "    \"\"\"\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].map(\n",
    "            apply_ocr,\n",
    "            batched=True,\n",
    "            batch_size=50,\n",
    "            writer_batch_size=50,\n",
    "            keep_in_memory=False,\n",
    "            )\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2436/2436 [2:02:06<00:00,  3.01s/ examples]  \n",
      "Map: 100%|██████████| 523/523 [25:15<00:00,  2.90s/ examples]\n",
      "Map: 100%|██████████| 523/523 [25:33<00:00,  2.93s/ examples]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = apply_ocr_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1728x2292>,\n",
       " 'doc_category': 'Letter',\n",
       " 'text': 'tobacco institute 1875 1 street northwest mighael j kerrigan washington dq song vice president 202 457 9800 b00 424 0876 state activities 202 4874888 january 24 1984 dear site enclosed please find uly aligned lobbyist regis tration 1983 85 mr n dean morgan algo accordance item 9 sees sceaeta form attached current list tobacco institute members assessed association dues 500 five hundred dollars per year questions ponuerning infot mation please feel free call office sincerely ry michael j enter fmm enclosures washington state public disclosure commission 403 evergreen plaza fj 42 olympia wa 98504 tnwl 0029158'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, die prüft, ob der Text leer ist\n",
    "def is_empty_string(example):\n",
    "    return example[\"text\"] == \"\"\n",
    "\n",
    "# Initialisieren eines Dictionaries zur Speicherung der leeren Beispiele und ihrer IDs\n",
    "empty_examples = {}\n",
    "\n",
    "# Durchlaufen der Splits und Sammeln der leeren Beispiele und ihrer IDs\n",
    "for split in processed_dataset.keys():\n",
    "    empty_examples[split] = [{\"id\": idx, \"example\": example} for idx, example in enumerate(processed_dataset[split]) if is_empty_string(example)]\n",
    "\n",
    "# Ausgabe der leeren Beispiele und ihrer IDs für jeden Split\n",
    "for split, examples in empty_examples.items():\n",
    "    print(f\"Leere Beispiele im '{split}'-Split:\")\n",
    "    for item in examples:\n",
    "        print(f\"ID: {item['id']}, Beispiel: {item['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2436/2436 [00:37<00:00, 64.51 examples/s] \n",
      "Map: 100%|██████████| 523/523 [00:05<00:00, 98.22 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Funktion zum Aktualisieren eines Beispiels\n",
    "def update_example(example, idx, target_idx):\n",
    "    if idx == target_idx:\n",
    "        example['text'] = \"no text found in document image with ocr!\"\n",
    "    return example\n",
    "\n",
    "# Anwendung der Aktualisierung auf das spezifische Beispiel\n",
    "target_indices = {'train': 343, 'validation': 98}  # Beispielhafte Ziel-Indizes\n",
    "\n",
    "for split, target_idx in target_indices.items():\n",
    "    processed_dataset[split] = processed_dataset[split].map(lambda x, idx: update_example(x, idx, target_idx), with_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no text found in document image with ocr!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][343][\"text\"]\n",
    "processed_dataset[\"validation\"][98][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der leeren Strings im 'train'-Split: 0\n",
      "Anzahl der leeren Strings im 'validation'-Split: 0\n",
      "Anzahl der leeren Strings im 'test'-Split: 0\n"
     ]
    }
   ],
   "source": [
    "# Funktion, die prüft, ob der Text leer ist\n",
    "def is_empty_string(example):\n",
    "    return example[\"text\"] == \"\"\n",
    "\n",
    "# Zählen der leeren Strings in jedem Split\n",
    "empty_counts = {}\n",
    "for split in processed_dataset.keys():\n",
    "    empty_count = sum(1 for example in processed_dataset[split] if is_empty_string(example))\n",
    "    empty_counts[split] = empty_count\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "for split, count in empty_counts.items():\n",
    "    print(f\"Anzahl der leeren Strings im '{split}'-Split: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 2436/2436 [00:50<00:00, 48.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 523/523 [00:09<00:00, 52.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 523/523 [00:10<00:00, 50.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset.save_to_disk(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
