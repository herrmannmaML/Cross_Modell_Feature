{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from PIL import Image\n",
    "\n",
    "# zeige keine Warnungen an\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.ocr_pipeline import OCRPreprocessor, OCRPostProcessor\n",
    "from src.utils import rotate_image, pil_to_cv, from_cv_to_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz initialisieren\n",
    "dataset = DatasetDict.load_from_disk(\"../data/interim_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "class OCRPipeline:\n",
    "    def __init__(self, image: Union[np.ndarray, Image.Image]):\n",
    "        \"\"\"OCR-Pipeline zu Vorbereitung des Dokumentes, \n",
    "        Extraktion des Textes und Aufbereitung des extrahierten Textes.\n",
    "\n",
    "        Args:\n",
    "            Args:\n",
    "            image (Union[np.ndarray, Image.Image]): Das Eingangsbild als NumPy-Array oder PIL.Image.Image.\n",
    "        \"\"\"\n",
    "        self.raw_image = image\n",
    "        self.preprocessed_image = None\n",
    "        self.ocr_output = \"\"\n",
    "\n",
    "    def preprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPreprocessor an, speichert das verarbeitete Bild.\"\"\"\n",
    "        preprocessor = OCRPreprocessor(self.raw_image)\n",
    "        preprocessor.cropping(buffer_size=10)\n",
    "        preprocessor.to_gray()\n",
    "        preprocessor.correct_skew()\n",
    "        preprocessor.sharpen(kernel_type=\"laplace_standard\")\n",
    "        preprocessor.opening(kernel=(1,1), iterations=2)\n",
    "        preprocessor.power_law_transform(gamma=2)\n",
    "        self.preprocessed_image = preprocessor.get_image()\n",
    "\n",
    "    def extract_text(self) -> None:\n",
    "        \"\"\"Wendet PyTesseract auf das vorverarbeitete Bild an und speichert den Text.\"\"\"\n",
    "        self.ocr_output = pytesseract.image_to_string(self.preprocessed_image)\n",
    "\n",
    "    def postprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPostProcessor auf den extrahierten Text an.\"\"\"\n",
    "        if self.ocr_output.strip():  # Prüft, ob `ocr_output` nicht leer ist\n",
    "            postprocessor = OCRPostProcessor(self.ocr_output)\n",
    "            # Anwenden verschiedener Methoden\n",
    "            postprocessor.identify_language()\n",
    "            postprocessor.remove_special_characters()\n",
    "            postprocessor.lowercase()\n",
    "            postprocessor.remove_stopwords()\n",
    "            postprocessor.remove_extra_spaces()\n",
    "            \n",
    "            # Aufbereiteten OCR-Output extrahieren\n",
    "            self.ocr_output = postprocessor.get_text()\n",
    "        else:\n",
    "            self.ocr_output = \"no text found in document image with ocr!\"\n",
    "\n",
    "    def get_output(self):\n",
    "        \"\"\"Gibt den aufbereiteten OCR-Output zurück.\"\"\"\n",
    "        return self.ocr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].add_column(\"text\", [\"\"]*len(dataset[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "# Erstellen eines Subsets mit 50 Beispielen aus einem Split\n",
    "subset = dataset[\"train\"].select(range(50))\n",
    "\n",
    "# Funktion zum Anwenden der OCR auf ein Subset\n",
    "def apply_ocr_to_subset(subset):\n",
    "    for i in tqdm(range(len(subset)), desc=f\"Processing subset\"):\n",
    "        example = subset[i]\n",
    "        \n",
    "        ocr_pipeline = OCRPipeline(example[\"image\"])\n",
    "        \n",
    "        ocr_pipeline.preprocess()\n",
    "        ocr_pipeline.extract_text()\n",
    "        ocr_pipeline.postprocess()\n",
    "        \n",
    "        subset = subset.map(\n",
    "            lambda example, idx: {\"text\": ocr_pipeline.get_output()} if idx == i else example,\n",
    "            with_indices=True\n",
    "        )\n",
    "        \n",
    "        del ocr_pipeline, example\n",
    "        gc.collect()\n",
    "        \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:01<00:00, 28.25 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:02<00:00, 19.25 examples/s]7s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 182.46 examples/s]s/it]\n",
      "Map: 100%|██████████| 50/50 [00:01<00:00, 31.00 examples/s]9s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 103.57 examples/s]s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 96.98 examples/s] s/it]\n",
      "Map: 100%|██████████| 50/50 [00:03<00:00, 15.84 examples/s]1s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 105.46 examples/s]s/it]\n",
      "Map: 100%|██████████| 50/50 [00:01<00:00, 49.04 examples/s]6s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 140.61 examples/s]s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 172.72 examples/s]7s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 146.65 examples/s]8s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 142.40 examples/s]3s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 191.22 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 179.37 examples/s]1s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 58.92 examples/s]84s/it]\n",
      "Map: 100%|██████████| 50/50 [00:01<00:00, 36.77 examples/s]69s/it]\n",
      "Map: 100%|██████████| 50/50 [00:02<00:00, 21.25 examples/s]69s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 92.03 examples/s]50s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 77.11 examples/s]32s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 122.68 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 181.89 examples/s]2s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 131.78 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 111.99 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 180.28 examples/s]6s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 172.48 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 148.94 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 156.99 examples/s]0s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 171.82 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 121.88 examples/s]4s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 93.38 examples/s]26s/it]\n",
      "Map: 100%|██████████| 50/50 [00:01<00:00, 43.08 examples/s]36s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 119.63 examples/s]0s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 139.72 examples/s]9s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 184.39 examples/s]3s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 93.16 examples/s]01s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 136.46 examples/s]2s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 68.16 examples/s]89s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 67.17 examples/s]23s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 103.70 examples/s]7s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 135.97 examples/s]8s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 128.56 examples/s]8s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 66.31 examples/s]25s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 101.59 examples/s]3s/it]\n",
      "Map: 100%|██████████| 50/50 [00:01<00:00, 38.15 examples/s]47s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 187.88 examples/s]2s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 157.63 examples/s]9s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 109.73 examples/s]7s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 123.39 examples/s]5s/it]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 156.60 examples/s]9s/it]\n",
      "Processing subset: 100%|██████████| 50/50 [05:10<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Anwenden der OCR auf das Subset\n",
    "processed_subset = apply_ocr_to_subset(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1728x2292 at 0xFFFF158D2910>, 'doc_category': 'Letter', 'text': 'tobacco institute 1875 1 street northwest mighael j kerrigan washington dq song vice president 202 457 9800 b00 424 0876 state activities 202 4874888 january 24 1984 dear site enclosed please find uly aligned lobbyist regis tration 1983 85 mr n dean morgan algo accordance item 9 sees sceaeta form attached current list tobacco institute members assessed association dues 500 five hundred dollars per year questions ponuerning infot mation please feel free call office sincerely ry michael j enter fmm enclosures washington state public disclosure commission 403 evergreen plaza fj 42 olympia wa 98504 tnwl 0029158'}\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1728x2292 at 0xFFFF158D1250>, 'doc_category': 'Report', 'text': 'communicating issues 1975 tobacco institute decided try little experiment previous decade battered media surgeon general report 1964 big news outisde major media washington c new york city response muted sut changed group institute executives hlt road conducted media breifings series oities around country found gratifying learned media genuinely interested say hungry news side gotten generally either networks wire services personal contact tobacco industry representative reporting briefings generally fair lead us conclude needed fulltime professional communicators staff continue carrying message around country later 1975 two people hired next year communications committee endorsed expansion program two communications professionals came board joined ti since traveled hundreds thousands miles conducted thousands tl1674 0794'}\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2560x3294 at 0xFFFF158D2950>, 'doc_category': 'Form', 'text': 'es 2 ssuecate supersedes 08 28 90 01 23 90 philip morris u lockout procedure basic job procedure 1 notify affected employees lockout tagout performed 2 shut equipment normal stopping pro cedure ensure sqaipment come complete atop 3 lock tag following potentially hazardous energy sources per park 500 lockout policy see attachment energy isolation locations ny electrical disconnects shall death serious injury could position locked tagged result disconnect arcing philip morris supplied locking device exploding equipment provides positive means physical load running electrical ly isolate energy equipment shock death could result frayed exposed im always stand side look away properly grounded wiring disconnect box activating unauthorized tampering switch minimize potential energized electrical circuits exposure electrical occur pie 2030114328 ie oe se ce ree 1 ar cat eccsinmonelbvemcie ripe bedt ar'}\n"
     ]
    }
   ],
   "source": [
    "# Überprüfen der Ergebnisse\n",
    "print(processed_subset[0])\n",
    "print(processed_subset[1])\n",
    "print(processed_subset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def apply_ocr_to_dataset(dataset: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Diese Methode wendet die OCR (Optical Character Recognition) auf alle Bilder in jedem Split (train, validation, test) eines Huggingface-Datensatzes an und fügt ein neues Feature hinzu, das den erkannten Text enthält.\n",
    "    \"\"\"\n",
    "    for split in dataset.keys():\n",
    "        for i in tqdm(range(len(dataset[split])), desc=f\"Processing {split}\"):\n",
    "            example = dataset[split][i]\n",
    "            \n",
    "            ocr_pipeline = OCRPipeline(example[\"image\"])\n",
    "            \n",
    "            ocr_pipeline.preprocess()\n",
    "            ocr_pipeline.extract_text()\n",
    "            ocr_pipeline.postprocess()\n",
    "            \n",
    "            dataset[split][i][\"text\"] = ocr_pipeline.get_output()\n",
    "            \n",
    "            del ocr_pipeline, example\n",
    "            gc.collect()\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 2436/2436 [2:04:18<00:00,  3.06s/it]  \n",
      "Processing validation: 100%|██████████| 523/523 [26:32<00:00,  3.04s/it]\n",
      "Processing test: 100%|██████████| 523/523 [26:03<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = apply_ocr_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1728x2292>,\n",
       " 'doc_category': 'Letter',\n",
       " 'text': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der leeren Strings im 'train'-Split: 2436\n",
      "Anzahl der leeren Strings im 'validation'-Split: 523\n",
      "Anzahl der leeren Strings im 'test'-Split: 523\n"
     ]
    }
   ],
   "source": [
    "# Funktion, die prüft, ob der Text leer ist\n",
    "def is_empty_string(example):\n",
    "    return example[\"text\"] == \"\"\n",
    "\n",
    "# Zählen der leeren Strings in jedem Split\n",
    "empty_counts = {}\n",
    "for split in processed_dataset.keys():\n",
    "    empty_count = sum(1 for example in processed_dataset[split] if is_empty_string(example))\n",
    "    empty_counts[split] = empty_count\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "for split, count in empty_counts.items():\n",
    "    print(f\"Anzahl der leeren Strings im '{split}'-Split: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 2436/2436 [01:15<00:00, 32.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 523/523 [00:10<00:00, 51.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 523/523 [00:22<00:00, 22.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset.save_to_disk(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
