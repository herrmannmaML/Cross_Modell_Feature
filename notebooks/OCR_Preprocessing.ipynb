{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from PIL import Image\n",
    "\n",
    "# zeige keine Warnungen an\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.ocr_pipeline import OCRPreprocessor, OCRPostProcessor\n",
    "from src.utils import rotate_image, pil_to_cv, from_cv_to_pil\n",
    "\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_cache_files(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Löscht alle Cache-Dateien mit der Endung '.arrow' im angegebenen Verzeichnis und seinen Unterverzeichnissen.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Das Verzeichnis, in dem nach Cache-Dateien gesucht werden soll.\n",
    "        \n",
    "    Gibt aus:\n",
    "        Die Gesamtgröße des freigegebenen Speicherplatzes in Gigabyte (GB).\n",
    "    \"\"\"\n",
    "    total_freed_space = 0  # Variable zur Speicherung der gesamten freigegebenen Speichergröße in Bytes\n",
    "\n",
    "    # Durchlaufen des Verzeichnisses und aller Unterordner\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Überprüfen, ob der Dateiname \"cache\" enthält und mit \".arrow\" endet\n",
    "            if \"cache\" in file and file.endswith(\".arrow\"):\n",
    "                file_path = os.path.join(root, file)  # Vollständigen Pfad der Datei erstellen\n",
    "                file_size = os.path.getsize(file_path)  # Größe der Datei in Bytes ermitteln\n",
    "                total_freed_space += file_size  # Speichergröße zur Gesamtsumme hinzufügen\n",
    "                os.remove(file_path)  # Datei löschen\n",
    "                print(f\"Gelöscht: {file_path}\")  # Bestätigung der gelöschten Datei ausgeben\n",
    "\n",
    "    # Umrechnen der freigegebenen Speichergröße von Bytes in Gigabyte\n",
    "    freed_space_gb = total_freed_space / (1024**3)\n",
    "\n",
    "    # Ausgabe der zusammengefassten Informationen\n",
    "    print(f\"Insgesamt {freed_space_gb:.2f} GB Speicherplatz freigegeben.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insgesamt 0.00 GB Speicherplatz freigegeben.\n"
     ]
    }
   ],
   "source": [
    "delete_cache_files(\"../data/interim_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz initialisieren\n",
    "dataset = DatasetDict.load_from_disk(\"../data/interim_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "class OCRPipeline:\n",
    "    def __init__(self, image: Union[np.ndarray, Image.Image]):\n",
    "        \"\"\"OCR-Pipeline zu Vorbereitung des Dokumentes, \n",
    "        Extraktion des Textes und Aufbereitung des extrahierten Textes.\n",
    "\n",
    "        Args:\n",
    "            Args:\n",
    "            image (Union[np.ndarray, Image.Image]): Das Eingangsbild als NumPy-Array oder PIL.Image.Image.\n",
    "        \"\"\"\n",
    "        self.raw_image = image\n",
    "        self.preprocessed_image = None\n",
    "        self.ocr_output = \"\"\n",
    "\n",
    "    def preprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPreprocessor an, speichert das verarbeitete Bild.\"\"\"\n",
    "        preprocessor = OCRPreprocessor(self.raw_image)\n",
    "        preprocessor.cropping(buffer_size=10)\n",
    "        preprocessor.to_gray()\n",
    "        preprocessor.correct_skew()\n",
    "        preprocessor.sharpen(kernel_type=\"laplace_standard\")\n",
    "        preprocessor.opening(kernel=(1,1), iterations=2)\n",
    "        preprocessor.power_law_transform(gamma=2)\n",
    "        self.preprocessed_image = preprocessor.get_image()\n",
    "\n",
    "    def extract_text(self) -> None:\n",
    "        \"\"\"Wendet PyTesseract auf das vorverarbeitete Bild an und speichert den Text.\"\"\"\n",
    "        self.ocr_output = pytesseract.image_to_string(self.preprocessed_image)\n",
    "\n",
    "    def postprocess(self) -> None:\n",
    "        \"\"\"Initialisiert und wendet den OCRPostProcessor auf den extrahierten Text an.\"\"\"\n",
    "        if self.ocr_output.strip():  # Prüft, ob `ocr_output` nicht leer ist\n",
    "            postprocessor = OCRPostProcessor(self.ocr_output)\n",
    "            # Anwenden verschiedener Methoden\n",
    "            postprocessor.identify_language()\n",
    "            postprocessor.remove_special_characters()\n",
    "            postprocessor.lowercase()\n",
    "            postprocessor.remove_stopwords()\n",
    "            postprocessor.remove_extra_spaces()\n",
    "            \n",
    "            # Aufbereiteten OCR-Output extrahieren\n",
    "            self.ocr_output = postprocessor.get_text()\n",
    "        else:\n",
    "            self.ocr_output = \"no text found in document image with ocr!\"\n",
    "\n",
    "    def get_output(self):\n",
    "        \"\"\"Gibt den aufbereiteten OCR-Output zurück.\"\"\"\n",
    "        return self.ocr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ocr_to_dataset(dataset: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Diese Methode wendet die OCR (Optical Character Recognition) auf alle Bilder in jedem Split (train, validation, test) eines Huggingface-Datensatzes an und fügt ein neues Feature hinzu, das den erkannten Text enthält.\n",
    "    \"\"\"\n",
    "    def add_ocr_text(example: dict) -> dict:\n",
    "        image = example['image']\n",
    "            \n",
    "        ocr_pipeline = OCRPipeline(image)\n",
    "            \n",
    "        ocr_pipeline.preprocess()\n",
    "\n",
    "        ocr_pipeline.extract_text()\n",
    "            \n",
    "        ocr_pipeline.postprocess()\n",
    "\n",
    "        example['text'] = ocr_pipeline.get_output()\n",
    "            \n",
    "        return example\n",
    "    \n",
    "     # Anwenden der Funktion auf jeden Split im Datensatz\n",
    "    #for split in dataset.keys():\n",
    "    dataset = dataset.map(add_ocr_text, keep_in_memory=False, batch_size=1)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  14%|█▍        | 72/523 [03:25<19:18,  2.57s/ examples]"
     ]
    }
   ],
   "source": [
    "test_dataset = apply_ocr_to_dataset(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#processed_dataset = apply_ocr_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfen ob kein string im Feature \"Text\" leer ist in allen drei Datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, die prüft, ob der Text leer ist\n",
    "def is_empty_string(example):\n",
    "    return example[\"text\"] == \"\"\n",
    "\n",
    "# Zählen der leeren Strings in jedem Split\n",
    "empty_counts = {}\n",
    "for split in dataset.keys():\n",
    "    empty_count = sum(1 for example in dataset[split] if is_empty_string(example))\n",
    "    empty_counts[split] = empty_count\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "for split, count in empty_counts.items():\n",
    "    print(f\"Anzahl der leeren Strings im '{split}'-Split: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset.save_to_disk(\"../data/processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
